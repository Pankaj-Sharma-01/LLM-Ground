{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Byte Pair Encoding (BPE) Implementation\n",
        "\n",
        "This notebook implements the Byte Pair Encoding algorithm using PyTorch for tokenization. BPE is a subword tokenization algorithm that iteratively merges the most frequent pairs of characters or subwords.\n",
        "\n",
        "## Features:\n",
        "- Complete BPE implementation with PyTorch\n",
        "- PDF text extraction using PyPDF\n",
        "- Vocabulary building and tokenization\n",
        "- Encoding and decoding functionality\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.8.0\n",
            "Available device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "import re\n",
        "import os\n",
        "from typing import List, Dict, Tuple, Set\n",
        "import pypdf\n",
        "from io import BytesIO\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"Available device:\", \"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PDF Text Extraction Utility\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully extracted text from /Users/maverick/pankaj/LLM-Ground/ux_law.pdf\n",
            "Total pages: 186\n",
            "Text length: 257256 characters\n"
          ]
        }
      ],
      "source": [
        "class PDFTextExtractor:\n",
        "    \"\"\"Utility class for extracting text from PDF files\"\"\"\n",
        "    \n",
        "    def __init__(self,):\n",
        "        self.extracted_text = \"\"\n",
        "    \n",
        "    def extract_text_from_pdf(self, pdf_path: str) -> str:\n",
        "        try:\n",
        "            with open(pdf_path, 'rb') as file:\n",
        "                pdf_reader = pypdf.PdfReader(file)\n",
        "                text = \"\"\n",
        "                \n",
        "                for page_num in range(len(pdf_reader.pages)):\n",
        "                    page = pdf_reader.pages[page_num]\n",
        "                    text += page.extract_text() + \"\\n\"\n",
        "                \n",
        "\n",
        "                self.extracted_text = self.preprocess_text(text)\n",
        "                \n",
        "                print(f\"Successfully extracted text from {pdf_path}\")\n",
        "                print(f\"Total pages: {len(pdf_reader.pages)}\")\n",
        "                print(f\"Text length: {len(text)} characters\")\n",
        "                \n",
        "                return text                \n",
        "        \n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting text from PDF: {e}\")\n",
        "            return \"\"\n",
        "    \n",
        "    \n",
        "    def preprocess_text(self, text: str) -> str:\n",
        "        # Remove extra whitespace and normalize\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = text.strip()\n",
        "        \n",
        "        # Remove special characters that might interfere with tokenization\n",
        "        text = re.sub(r'[^\\w\\s.,!?;:()\\-]', '', text)\n",
        "        \n",
        "        return text\n",
        "\n",
        "pdf_extractor = PDFTextExtractor()\n",
        "text = pdf_extractor.extract_text_from_pdf('/Users/maverick/pankaj/LLM-Ground/ux_law.pdf')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Byte Pair Encoding Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BytePairEncoder:\n",
        "    \"\"\"\n",
        "    Byte Pair Encoding implementation using PyTorch\n",
        "    \n",
        "    BPE is a subword tokenization algorithm that iteratively merges the most frequent\n",
        "    pairs of characters or subwords to create a vocabulary.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size: int = 1000):\n",
        "        \"\"\"\n",
        "        Initialize BPE encoder\n",
        "        \n",
        "        Args:\n",
        "            vocab_size (int): Target vocabulary size\n",
        "        \"\"\"\n",
        "        self.vocab_size = vocab_size\n",
        "        self.word_freqs = Counter()\n",
        "        self.vocab = {}\n",
        "        self.merges = []\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        \n",
        "    def get_word_freqs(self, text: str) -> Counter:\n",
        "        \"\"\"\n",
        "        Get word frequencies from text\n",
        "        \n",
        "        Args:\n",
        "            text (str): Input text\n",
        "            \n",
        "        Returns:\n",
        "            Counter: Word frequency counter\n",
        "        \"\"\"\n",
        "        # Split text into words and add end-of-word token\n",
        "        words = text.split()\n",
        "        word_freqs = Counter()\n",
        "        \n",
        "        for word in words:\n",
        "            # Add end-of-word token\n",
        "            word_freqs[' '.join(list(word)) + ' </w>'] += 1\n",
        "            \n",
        "        return word_freqs\n",
        "    \n",
        "    def get_stats(self, vocab: Dict[str, int]) -> Dict[Tuple[str, str], int]:\n",
        "        \"\"\"\n",
        "        Get statistics of character pairs\n",
        "        \n",
        "        Args:\n",
        "            vocab (Dict[str, int]): Current vocabulary with frequencies\n",
        "            \n",
        "        Returns:\n",
        "            Dict[Tuple[str, str], int]: Pair frequencies\n",
        "        \"\"\"\n",
        "        pairs = defaultdict(int)\n",
        "        \n",
        "        for word, freq in vocab.items():\n",
        "            symbols = word.split()\n",
        "            \n",
        "            for i in range(len(symbols) - 1):\n",
        "                pairs[(symbols[i], symbols[i + 1])] += freq\n",
        "                \n",
        "        return pairs\n",
        "    \n",
        "    def merge_vocab(self, pair: Tuple[str, str], vocab: Dict[str, int]) -> Dict[str, int]:\n",
        "        \"\"\"\n",
        "        Merge a pair in the vocabulary\n",
        "        \n",
        "        Args:\n",
        "            pair (Tuple[str, str]): Pair to merge\n",
        "            vocab (Dict[str, int]): Current vocabulary\n",
        "            \n",
        "        Returns:\n",
        "            Dict[str, int]: Updated vocabulary\n",
        "        \"\"\"\n",
        "        bigram = re.escape(' '.join(pair))\n",
        "        p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "        \n",
        "        new_vocab = {}\n",
        "        for word in vocab:\n",
        "            new_word = p.sub(''.join(pair), word)\n",
        "            new_vocab[new_word] = vocab[word]\n",
        "            \n",
        "        return new_vocab\n",
        "    \n",
        "    def train(self, text: str) -> None:\n",
        "        \"\"\"\n",
        "        Train BPE model on text\n",
        "        \n",
        "        Args:\n",
        "            text (str): Training text\n",
        "        \"\"\"\n",
        "        print(\"Starting BPE training...\")\n",
        "        \n",
        "        # Get initial word frequencies\n",
        "        self.word_freqs = self.get_word_freqs(text)\n",
        "        print(f\"Initial vocabulary size: {len(self.word_freqs)}\")\n",
        "        \n",
        "        # Initialize vocabulary with character-level tokens\n",
        "        vocab = dict(self.word_freqs)\n",
        "        \n",
        "        # Get initial character set\n",
        "        chars = set()\n",
        "        for word in vocab.keys():\n",
        "            chars.update(word.split())\n",
        "        \n",
        "        # Add special tokens\n",
        "        chars.add('<unk>')\n",
        "        chars.add('<pad>')\n",
        "        chars.add('<s>')\n",
        "        chars.add('</s>')\n",
        "        \n",
        "        # Initialize vocabulary with characters\n",
        "        for char in chars:\n",
        "            if char not in vocab:\n",
        "                vocab[char] = 0\n",
        "        \n",
        "        print(f\"Character vocabulary size: {len(chars)}\")\n",
        "        \n",
        "        # Perform BPE merges\n",
        "        num_merges = self.vocab_size - len(chars)\n",
        "        \n",
        "        for i in range(num_merges):\n",
        "            pairs = self.get_stats(vocab)\n",
        "            if not pairs:\n",
        "                break\n",
        "                \n",
        "            # Get most frequent pair\n",
        "            best_pair = max(pairs, key=pairs.get)\n",
        "            \n",
        "            # Merge the pair\n",
        "            vocab = self.merge_vocab(best_pair, vocab)\n",
        "            self.merges.append(best_pair)\n",
        "            \n",
        "            if (i + 1) % 100 == 0:\n",
        "                print(f\"Completed {i + 1} merges. Current vocab size: {len(vocab)}\")\n",
        "        \n",
        "        self.vocab = vocab\n",
        "        print(f\"Training completed. Final vocabulary size: {len(self.vocab)}\")\n",
        "        print(f\"Number of merges performed: {len(self.merges)}\")\n",
        "    \n",
        "    def encode_word(self, word: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Encode a single word using BPE\n",
        "        \n",
        "        Args:\n",
        "            word (str): Word to encode\n",
        "            \n",
        "        Returns:\n",
        "            List[str]: List of BPE tokens\n",
        "        \"\"\"\n",
        "        if word in self.vocab:\n",
        "            return word.split()\n",
        "        \n",
        "        # Apply merges in order\n",
        "        word = ' '.join(list(word)) + ' </w>'\n",
        "        \n",
        "        for pair in self.merges:\n",
        "            bigram = re.escape(' '.join(pair))\n",
        "            p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "            word = p.sub(''.join(pair), word)\n",
        "        \n",
        "        return word.split()\n",
        "    \n",
        "    def encode(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Encode text using BPE\n",
        "        \n",
        "        Args:\n",
        "            text (str): Text to encode\n",
        "            \n",
        "        Returns:\n",
        "            List[str]: List of BPE tokens\n",
        "        \"\"\"\n",
        "        words = text.split()\n",
        "        tokens = []\n",
        "        \n",
        "        for word in words:\n",
        "            tokens.extend(self.encode_word(word))\n",
        "        \n",
        "        return tokens\n",
        "    \n",
        "    def decode(self, tokens: List[str]) -> str:\n",
        "        \"\"\"\n",
        "        Decode BPE tokens back to text\n",
        "        \n",
        "        Args:\n",
        "            tokens (List[str]): BPE tokens\n",
        "            \n",
        "        Returns:\n",
        "            str: Decoded text\n",
        "        \"\"\"\n",
        "        # Join tokens and remove end-of-word markers\n",
        "        text = ''.join(tokens)\n",
        "        text = text.replace('</w>', ' ')\n",
        "        \n",
        "        return text.strip()\n",
        "    \n",
        "    def get_vocab_stats(self) -> Dict[str, int]:\n",
        "        \"\"\"\n",
        "        Get vocabulary statistics\n",
        "        \n",
        "        Returns:\n",
        "            Dict[str, int]: Vocabulary statistics\n",
        "        \"\"\"\n",
        "        stats = {\n",
        "            'total_vocab_size': len(self.vocab),\n",
        "            'number_of_merges': len(self.merges),\n",
        "            'most_frequent_tokens': dict(Counter(self.vocab).most_common(10))\n",
        "        }\n",
        "        return stats\n",
        "\n",
        "# Initialize BPE encoder\n",
        "bpe_encoder = BytePairEncoder(vocab_size=1000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PyTorch Integration for Tokenization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PyTorchTokenizer(nn.Module):\n",
        "    \"\"\"\n",
        "    PyTorch module for BPE tokenization with tensor operations\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, bpe_encoder: BytePairEncoder):\n",
        "        \"\"\"\n",
        "        Initialize PyTorch tokenizer\n",
        "        \n",
        "        Args:\n",
        "            bpe_encoder (BytePairEncoder): Trained BPE encoder\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.bpe_encoder = bpe_encoder\n",
        "        self.vocab_size = len(bpe_encoder.vocab)\n",
        "        \n",
        "        # Create token to index mapping\n",
        "        self.token_to_idx = {token: idx for idx, token in enumerate(bpe_encoder.vocab.keys())}\n",
        "        self.idx_to_token = {idx: token for token, idx in self.token_to_idx.items()}\n",
        "        \n",
        "        # Special token indices\n",
        "        self.unk_idx = self.token_to_idx.get('<unk>', 0)\n",
        "        self.pad_idx = self.token_to_idx.get('<pad>', 1)\n",
        "        self.sos_idx = self.token_to_idx.get('<s>', 2)\n",
        "        self.eos_idx = self.token_to_idx.get('</s>', 3)\n",
        "        \n",
        "    def encode_to_tensor(self, text: str, max_length: int = 512) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Encode text to PyTorch tensor\n",
        "        \n",
        "        Args:\n",
        "            text (str): Input text\n",
        "            max_length (int): Maximum sequence length\n",
        "            \n",
        "        Returns:\n",
        "            torch.Tensor: Token indices tensor\n",
        "        \"\"\"\n",
        "        tokens = self.bpe_encoder.encode(text)\n",
        "        token_ids = []\n",
        "        \n",
        "        for token in tokens:\n",
        "            if token in self.token_to_idx:\n",
        "                token_ids.append(self.token_to_idx[token])\n",
        "            else:\n",
        "                token_ids.append(self.unk_idx)\n",
        "        \n",
        "        # Truncate or pad to max_length\n",
        "        if len(token_ids) > max_length:\n",
        "            token_ids = token_ids[:max_length]\n",
        "        else:\n",
        "            token_ids.extend([self.pad_idx] * (max_length - len(token_ids)))\n",
        "        \n",
        "        return torch.tensor(token_ids, dtype=torch.long)\n",
        "    \n",
        "    def decode_from_tensor(self, tensor: torch.Tensor) -> str:\n",
        "        \"\"\"\n",
        "        Decode PyTorch tensor back to text\n",
        "        \n",
        "        Args:\n",
        "            tensor (torch.Tensor): Token indices tensor\n",
        "            \n",
        "        Returns:\n",
        "            str: Decoded text\n",
        "        \"\"\"\n",
        "        token_ids = tensor.tolist()\n",
        "        tokens = []\n",
        "        \n",
        "        for token_id in token_ids:\n",
        "            if token_id in self.idx_to_token:\n",
        "                token = self.idx_to_token[token_id]\n",
        "                if token not in ['<pad>', '<s>', '</s>']:\n",
        "                    tokens.append(token)\n",
        "        \n",
        "        return self.bpe_encoder.decode(tokens)\n",
        "    \n",
        "    def get_embedding_layer(self, embedding_dim: int = 128) -> nn.Embedding:\n",
        "        \"\"\"\n",
        "        Create embedding layer for tokens\n",
        "        \n",
        "        Args:\n",
        "            embedding_dim (int): Embedding dimension\n",
        "            \n",
        "        Returns:\n",
        "            nn.Embedding: PyTorch embedding layer\n",
        "        \"\"\"\n",
        "        return nn.Embedding(self.vocab_size, embedding_dim, padding_idx=self.pad_idx)\n",
        "    \n",
        "    def forward(self, text: str, max_length: int = 512) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass for tokenization\n",
        "        \n",
        "        Args:\n",
        "            text (str): Input text\n",
        "            max_length (int): Maximum sequence length\n",
        "            \n",
        "        Returns:\n",
        "            torch.Tensor: Token indices tensor\n",
        "        \"\"\"\n",
        "        return self.encode_to_tensor(text, max_length)\n",
        "\n",
        "# Initialize PyTorch tokenizer (will be used after BPE training)\n",
        "pytorch_tokenizer = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
