{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Byte Pair Encoding (BPE) Implementation\n",
        "\n",
        "This notebook implements the Byte Pair Encoding algorithm using PyTorch for tokenization. BPE is a subword tokenization algorithm that iteratively merges the most frequent pairs of characters or subwords.\n",
        "\n",
        "## Features:\n",
        "- Complete BPE implementation with PyTorch\n",
        "- PDF text extraction using PyPDF\n",
        "- Vocabulary building and tokenization\n",
        "- Encoding and decoding functionality\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.8.0\n",
            "Available device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "import re\n",
        "import os\n",
        "from typing import List, Dict, Tuple, Set\n",
        "import pypdf\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"Available device:\", \"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PDF Text Extraction Utility\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully extracted text from /Users/maverick/pankaj/LLM-Ground/ux_law.pdf\n",
            "Total pages: 186\n",
            "Text length: 257256 characters\n"
          ]
        }
      ],
      "source": [
        "class PDFTextExtractor:\n",
        "    \"\"\"Utility class for extracting text from PDF files\"\"\"\n",
        "    \n",
        "    def __init__(self,):\n",
        "        self.extracted_text = \"\"\n",
        "    \n",
        "    def extract_text_from_pdf(self, pdf_path: str) -> str:\n",
        "        try:\n",
        "            with open(pdf_path, 'rb') as file:\n",
        "                pdf_reader = pypdf.PdfReader(file)\n",
        "                text = \"\"\n",
        "                \n",
        "                for page_num in range(len(pdf_reader.pages)):\n",
        "                    page = pdf_reader.pages[page_num]\n",
        "                    text += page.extract_text() + \"\\n\"\n",
        "                \n",
        "\n",
        "                self.extracted_text = self.preprocess_text(text)\n",
        "                \n",
        "                print(f\"Successfully extracted text from {pdf_path}\")\n",
        "                print(f\"Total pages: {len(pdf_reader.pages)}\")\n",
        "                print(f\"Text length: {len(text)} characters\")\n",
        "                \n",
        "                return text                \n",
        "        \n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting text from PDF: {e}\")\n",
        "            return \"\"\n",
        "    \n",
        "    \n",
        "    def preprocess_text(self, text: str) -> str:\n",
        "        # Remove extra whitespace and normalize\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = text.strip()\n",
        "        \n",
        "        # Remove special characters that might interfere with tokenization\n",
        "        text = re.sub(r'[^\\w\\s.,!?;:()\\-]', '', text)\n",
        "        \n",
        "        return text\n",
        "\n",
        "pdf_extractor = PDFTextExtractor()\n",
        "text = pdf_extractor.extract_text_from_pdf('/Users/maverick/pankaj/LLM-Ground/ux_law.pdf')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Byte Pair Encoding Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BytePairEncoder:\n",
        "    \"\"\"\n",
        "    Byte Pair Encoding implementation using PyTorch\n",
        "    \n",
        "    BPE is a subword tokenization algorithm that iteratively merges the most frequent\n",
        "    pairs of characters or subwords to create a vocabulary.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size: int = 1000):\n",
        "        \"\"\"\n",
        "        Initialize BPE encoder\n",
        "        \n",
        "        Args:\n",
        "            vocab_size (int): Target vocabulary size\n",
        "        \"\"\"\n",
        "        self.vocab_size = vocab_size\n",
        "        self.word_freqs = Counter()\n",
        "        self.vocab = {}\n",
        "        self.merges = []\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        \n",
        "    def get_word_freqs(self, text: str) -> Counter:\n",
        "        \"\"\"\n",
        "        Get word frequencies from text\n",
        "        \n",
        "        Args:\n",
        "            text (str): Input text\n",
        "            \n",
        "        Returns:\n",
        "            Counter: Word frequency counter\n",
        "        \"\"\"\n",
        "        # Split text into words and add end-of-word token\n",
        "        words = text.split()\n",
        "        word_freqs = Counter()\n",
        "        \n",
        "        for word in words:\n",
        "            # Add end-of-word token\n",
        "            word_freqs[' '.join(list(word)) + ' </w>'] += 1\n",
        "            \n",
        "        return word_freqs\n",
        "    \n",
        "    def get_stats(self, vocab: Dict[str, int]) -> Dict[Tuple[str, str], int]:\n",
        "        \"\"\"\n",
        "        Get statistics of character pairs\n",
        "        \n",
        "        Args:\n",
        "            vocab (Dict[str, int]): Current vocabulary with frequencies\n",
        "            \n",
        "        Returns:\n",
        "            Dict[Tuple[str, str], int]: Pair frequencies\n",
        "        \"\"\"\n",
        "        pairs = defaultdict(int)\n",
        "        \n",
        "        for word, freq in vocab.items():\n",
        "            symbols = word.split()\n",
        "            \n",
        "            for i in range(len(symbols) - 1):\n",
        "                pairs[(symbols[i], symbols[i + 1])] += freq\n",
        "                \n",
        "        return pairs\n",
        "    \n",
        "    def merge_vocab(self, pair: Tuple[str, str], vocab: Dict[str, int]) -> Dict[str, int]:\n",
        "        \"\"\"\n",
        "        Merge a pair in the vocabulary\n",
        "        \n",
        "        Args:\n",
        "            pair (Tuple[str, str]): Pair to merge\n",
        "            vocab (Dict[str, int]): Current vocabulary\n",
        "            \n",
        "        Returns:\n",
        "            Dict[str, int]: Updated vocabulary\n",
        "        \"\"\"\n",
        "        bigram = re.escape(' '.join(pair))\n",
        "        p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "        \n",
        "        new_vocab = {}\n",
        "        for word in vocab:\n",
        "            new_word = p.sub(''.join(pair), word)\n",
        "            new_vocab[new_word] = vocab[word]\n",
        "            \n",
        "        return new_vocab\n",
        "    \n",
        "    def train(self, text: str) -> None:\n",
        "        \"\"\"\n",
        "        Train BPE model on text\n",
        "        \n",
        "        Args:\n",
        "            text (str): Training text\n",
        "        \"\"\"\n",
        "        print(\"Starting BPE training...\")\n",
        "        \n",
        "        # Get initial word frequencies\n",
        "        self.word_freqs = self.get_word_freqs(text)\n",
        "        print(f\"Initial vocabulary size: {len(self.word_freqs)}\")\n",
        "        \n",
        "        # Initialize vocabulary with character-level tokens\n",
        "        vocab = dict(self.word_freqs)\n",
        "        \n",
        "        # Get initial character set\n",
        "        chars = set()\n",
        "        for word in vocab.keys():\n",
        "            chars.update(word.split())\n",
        "        \n",
        "        # Add special tokens\n",
        "        chars.add('<unk>')\n",
        "        chars.add('<pad>')\n",
        "        chars.add('<s>')\n",
        "        chars.add('</s>')\n",
        "        \n",
        "        # Initialize vocabulary with characters\n",
        "        for char in chars:\n",
        "            if char not in vocab:\n",
        "                vocab[char] = 0\n",
        "        \n",
        "        print(f\"Character vocabulary size: {len(chars)}\")\n",
        "        \n",
        "        # Perform BPE merges\n",
        "        num_merges = self.vocab_size - len(chars)\n",
        "        \n",
        "        for i in range(num_merges):\n",
        "            pairs = self.get_stats(vocab)\n",
        "            if not pairs:\n",
        "                break\n",
        "                \n",
        "            # Get most frequent pair\n",
        "            best_pair = max(pairs, key=pairs.get)\n",
        "            \n",
        "            # Merge the pair\n",
        "            vocab = self.merge_vocab(best_pair, vocab)\n",
        "            self.merges.append(best_pair)\n",
        "            \n",
        "            if (i + 1) % 100 == 0:\n",
        "                print(f\"Completed {i + 1} merges. Current vocab size: {len(vocab)}\")\n",
        "        \n",
        "        self.vocab = vocab\n",
        "        print(f\"Training completed. Final vocabulary size: {len(self.vocab)}\")\n",
        "        print(f\"Number of merges performed: {len(self.merges)}\")\n",
        "    \n",
        "    def encode_word(self, word: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Encode a single word using BPE\n",
        "        \n",
        "        Args:\n",
        "            word (str): Word to encode\n",
        "            \n",
        "        Returns:\n",
        "            List[str]: List of BPE tokens\n",
        "        \"\"\"\n",
        "        if word in self.vocab:\n",
        "            return word.split()\n",
        "        \n",
        "        # Apply merges in order\n",
        "        word = ' '.join(list(word)) + ' </w>'\n",
        "        \n",
        "        for pair in self.merges:\n",
        "            bigram = re.escape(' '.join(pair))\n",
        "            p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "            word = p.sub(''.join(pair), word)\n",
        "        \n",
        "        return word.split()\n",
        "    \n",
        "    def encode(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Encode text using BPE\n",
        "        \n",
        "        Args:\n",
        "            text (str): Text to encode\n",
        "            \n",
        "        Returns:\n",
        "            List[str]: List of BPE tokens\n",
        "        \"\"\"\n",
        "        words = text.split()\n",
        "        tokens = []\n",
        "        \n",
        "        for word in words:\n",
        "            tokens.extend(self.encode_word(word))\n",
        "        \n",
        "        return tokens\n",
        "    \n",
        "    def decode(self, tokens: List[str]) -> str:\n",
        "        \"\"\"\n",
        "        Decode BPE tokens back to text\n",
        "        \n",
        "        Args:\n",
        "            tokens (List[str]): BPE tokens\n",
        "            \n",
        "        Returns:\n",
        "            str: Decoded text\n",
        "        \"\"\"\n",
        "        # Join tokens and remove end-of-word markers\n",
        "        text = ''.join(tokens)\n",
        "        text = text.replace('</w>', ' ')\n",
        "        \n",
        "        return text.strip()\n",
        "    \n",
        "    def get_vocab_stats(self) -> Dict[str, int]:\n",
        "        \"\"\"\n",
        "        Get vocabulary statistics\n",
        "        \n",
        "        Returns:\n",
        "            Dict[str, int]: Vocabulary statistics\n",
        "        \"\"\"\n",
        "        stats = {\n",
        "            'total_vocab_size': len(self.vocab),\n",
        "            'number_of_merges': len(self.merges),\n",
        "            'most_frequent_tokens': dict(Counter(self.vocab).most_common(10))\n",
        "        }\n",
        "        return stats\n",
        "\n",
        "# Initialize BPE encoder\n",
        "bpe_encoder = BytePairEncoder(vocab_size=1000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PyTorch Integration for Tokenization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PyTorchTokenizer(nn.Module):\n",
        "    \"\"\"\n",
        "    PyTorch module for BPE tokenization with tensor operations\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, bpe_encoder: BytePairEncoder):\n",
        "        \"\"\"\n",
        "        Initialize PyTorch tokenizer\n",
        "        \n",
        "        Args:\n",
        "            bpe_encoder (BytePairEncoder): Trained BPE encoder\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.bpe_encoder = bpe_encoder\n",
        "        self.vocab_size = len(bpe_encoder.vocab)\n",
        "        \n",
        "        # Create token to index mapping\n",
        "        self.token_to_idx = {token: idx for idx, token in enumerate(bpe_encoder.vocab.keys())}\n",
        "        self.idx_to_token = {idx: token for token, idx in self.token_to_idx.items()}\n",
        "        \n",
        "        # Special token indices\n",
        "        self.unk_idx = self.token_to_idx.get('<unk>', 0)\n",
        "        self.pad_idx = self.token_to_idx.get('<pad>', 1)\n",
        "        self.sos_idx = self.token_to_idx.get('<s>', 2)\n",
        "        self.eos_idx = self.token_to_idx.get('</s>', 3)\n",
        "        \n",
        "    def encode_to_tensor(self, text: str, max_length: int = 512) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Encode text to PyTorch tensor\n",
        "        \n",
        "        Args:\n",
        "            text (str): Input text\n",
        "            max_length (int): Maximum sequence length\n",
        "            \n",
        "        Returns:\n",
        "            torch.Tensor: Token indices tensor\n",
        "        \"\"\"\n",
        "        tokens = self.bpe_encoder.encode(text)\n",
        "        token_ids = []\n",
        "        \n",
        "        for token in tokens:\n",
        "            if token in self.token_to_idx:\n",
        "                token_ids.append(self.token_to_idx[token])\n",
        "            else:\n",
        "                token_ids.append(self.unk_idx)\n",
        "        \n",
        "        # Truncate or pad to max_length\n",
        "        if len(token_ids) > max_length:\n",
        "            token_ids = token_ids[:max_length]\n",
        "        else:\n",
        "            token_ids.extend([self.pad_idx] * (max_length - len(token_ids)))\n",
        "        \n",
        "        return torch.tensor(token_ids, dtype=torch.long)\n",
        "    \n",
        "    def decode_from_tensor(self, tensor: torch.Tensor) -> str:\n",
        "        \"\"\"\n",
        "        Decode PyTorch tensor back to text\n",
        "        \n",
        "        Args:\n",
        "            tensor (torch.Tensor): Token indices tensor\n",
        "            \n",
        "        Returns:\n",
        "            str: Decoded text\n",
        "        \"\"\"\n",
        "        token_ids = tensor.tolist()\n",
        "        tokens = []\n",
        "        \n",
        "        for token_id in token_ids:\n",
        "            if token_id in self.idx_to_token:\n",
        "                token = self.idx_to_token[token_id]\n",
        "                if token not in ['<pad>', '<s>', '</s>']:\n",
        "                    tokens.append(token)\n",
        "        \n",
        "        return self.bpe_encoder.decode(tokens)\n",
        "    \n",
        "    def get_embedding_layer(self, embedding_dim: int = 128) -> nn.Embedding:\n",
        "        \"\"\"\n",
        "        Create embedding layer for tokens\n",
        "        \n",
        "        Args:\n",
        "            embedding_dim (int): Embedding dimension\n",
        "            \n",
        "        Returns:\n",
        "            nn.Embedding: PyTorch embedding layer\n",
        "        \"\"\"\n",
        "        return nn.Embedding(self.vocab_size, embedding_dim, padding_idx=self.pad_idx)\n",
        "    \n",
        "    def forward(self, text: str, max_length: int = 512) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass for tokenization\n",
        "        \n",
        "        Args:\n",
        "            text (str): Input text\n",
        "            max_length (int): Maximum sequence length\n",
        "            \n",
        "        Returns:\n",
        "            torch.Tensor: Token indices tensor\n",
        "        \"\"\"\n",
        "        return self.encode_to_tensor(text, max_length)\n",
        "\n",
        "# Initialize PyTorch tokenizer (will be used after BPE training)\n",
        "pytorch_tokenizer = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Usage Examples and Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage with sample text\n",
        "sample_text = \"\"\"\n",
        "Byte Pair Encoding is a subword tokenization algorithm that iteratively merges \n",
        "the most frequent pairs of characters or subwords. It is widely used in \n",
        "natural language processing for handling out-of-vocabulary words and \n",
        "reducing vocabulary size while maintaining semantic meaning.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Sample text:\")\n",
        "print(sample_text)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Train BPE on sample text\n",
        "print(\"Training BPE encoder...\")\n",
        "bpe_encoder.train(sample_text)\n",
        "\n",
        "# Get vocabulary statistics\n",
        "stats = bpe_encoder.get_vocab_stats()\n",
        "print(f\"\\nVocabulary Statistics:\")\n",
        "print(f\"Total vocabulary size: {stats['total_vocab_size']}\")\n",
        "print(f\"Number of merges: {stats['number_of_merges']}\")\n",
        "print(f\"Most frequent tokens: {list(stats['most_frequent_tokens'].keys())[:10]}\")\n",
        "\n",
        "# Initialize PyTorch tokenizer\n",
        "pytorch_tokenizer = PyTorchTokenizer(bpe_encoder)\n",
        "print(f\"\\nPyTorch tokenizer initialized with vocabulary size: {pytorch_tokenizer.vocab_size}\")\n",
        "\n",
        "# Test encoding and decoding\n",
        "test_text = \"Byte Pair Encoding algorithm\"\n",
        "print(f\"\\nTest text: '{test_text}'\")\n",
        "\n",
        "# Encode using BPE\n",
        "tokens = bpe_encoder.encode(test_text)\n",
        "print(f\"BPE tokens: {tokens}\")\n",
        "\n",
        "# Encode to PyTorch tensor\n",
        "tensor = pytorch_tokenizer.encode_to_tensor(test_text, max_length=20)\n",
        "print(f\"PyTorch tensor shape: {tensor.shape}\")\n",
        "print(f\"Token indices: {tensor.tolist()}\")\n",
        "\n",
        "# Decode back to text\n",
        "decoded_text = pytorch_tokenizer.decode_from_tensor(tensor)\n",
        "print(f\"Decoded text: '{decoded_text}'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PDF Processing Example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example function to process PDF files\n",
        "def process_pdf_with_bpe(pdf_path: str, vocab_size: int = 2000):\n",
        "    \"\"\"\n",
        "    Process a PDF file and train BPE on its content\n",
        "    \n",
        "    Args:\n",
        "        pdf_path (str): Path to the PDF file\n",
        "        vocab_size (int): Target vocabulary size for BPE\n",
        "    \"\"\"\n",
        "    print(f\"Processing PDF: {pdf_path}\")\n",
        "    \n",
        "    # Extract text from PDF\n",
        "    extracted_text = pdf_extractor.extract_text_from_pdf(pdf_path)\n",
        "    \n",
        "    if not extracted_text:\n",
        "        print(\"No text extracted from PDF\")\n",
        "        return None\n",
        "    \n",
        "    # Preprocess text\n",
        "    processed_text = pdf_extractor.preprocess_text(extracted_text)\n",
        "    print(f\"Processed text length: {len(processed_text)} characters\")\n",
        "    \n",
        "    # Create new BPE encoder for this document\n",
        "    doc_bpe_encoder = BytePairEncoder(vocab_size=vocab_size)\n",
        "    \n",
        "    # Train BPE on the document\n",
        "    doc_bpe_encoder.train(processed_text)\n",
        "    \n",
        "    # Create PyTorch tokenizer\n",
        "    doc_pytorch_tokenizer = PyTorchTokenizer(doc_bpe_encoder)\n",
        "    \n",
        "    # Show some statistics\n",
        "    stats = doc_bpe_encoder.get_vocab_stats()\n",
        "    print(f\"\\nDocument BPE Statistics:\")\n",
        "    print(f\"Vocabulary size: {stats['total_vocab_size']}\")\n",
        "    print(f\"Number of merges: {stats['number_of_merges']}\")\n",
        "    \n",
        "    # Test tokenization on a sample\n",
        "    sample_sentences = processed_text.split('.')[:3]  # First 3 sentences\n",
        "    print(f\"\\nTokenization examples:\")\n",
        "    \n",
        "    for i, sentence in enumerate(sample_sentences):\n",
        "        if sentence.strip():\n",
        "            tokens = doc_bpe_encoder.encode(sentence.strip())\n",
        "            tensor = doc_pytorch_tokenizer.encode_to_tensor(sentence.strip(), max_length=50)\n",
        "            decoded = doc_pytorch_tokenizer.decode_from_tensor(tensor)\n",
        "            \n",
        "            print(f\"\\nSentence {i+1}:\")\n",
        "            print(f\"Original: {sentence.strip()[:100]}...\")\n",
        "            print(f\"Tokens: {tokens[:10]}...\")  # Show first 10 tokens\n",
        "            print(f\"Tensor shape: {tensor.shape}\")\n",
        "            print(f\"Decoded: {decoded[:100]}...\")\n",
        "    \n",
        "    return doc_bpe_encoder, doc_pytorch_tokenizer\n",
        "\n",
        "# Example usage (uncomment when you have a PDF file)\n",
        "# pdf_path = \"path/to/your/document.pdf\"\n",
        "# bpe_encoder, pytorch_tokenizer = process_pdf_with_bpe(pdf_path)\n",
        "\n",
        "print(\"PDF processing function ready!\")\n",
        "print(\"To use with your PDF file:\")\n",
        "print(\"1. Place your PDF file in the TOKENIZATION folder\")\n",
        "print(\"2. Update the pdf_path variable with the correct filename\")\n",
        "print(\"3. Uncomment and run the process_pdf_with_bpe() function call\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization and Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_bpe_stats(bpe_encoder: BytePairEncoder):\n",
        "    \"\"\"\n",
        "    Visualize BPE statistics and token distribution\n",
        "    \n",
        "    Args:\n",
        "        bpe_encoder (BytePairEncoder): Trained BPE encoder\n",
        "    \"\"\"\n",
        "    stats = bpe_encoder.get_vocab_stats()\n",
        "    \n",
        "    # Create visualizations\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle('BPE Tokenization Analysis', fontsize=16)\n",
        "    \n",
        "    # 1. Most frequent tokens\n",
        "    most_frequent = stats['most_frequent_tokens']\n",
        "    tokens = list(most_frequent.keys())[:15]\n",
        "    frequencies = list(most_frequent.values())[:15]\n",
        "    \n",
        "    axes[0, 0].bar(range(len(tokens)), frequencies)\n",
        "    axes[0, 0].set_title('Most Frequent Tokens')\n",
        "    axes[0, 0].set_xlabel('Token Rank')\n",
        "    axes[0, 0].set_ylabel('Frequency')\n",
        "    axes[0, 0].set_xticks(range(len(tokens)))\n",
        "    axes[0, 0].set_xticklabels(tokens, rotation=45, ha='right')\n",
        "    \n",
        "    # 2. Token length distribution\n",
        "    token_lengths = [len(token) for token in bpe_encoder.vocab.keys()]\n",
        "    axes[0, 1].hist(token_lengths, bins=20, alpha=0.7, color='skyblue')\n",
        "    axes[0, 1].set_title('Token Length Distribution')\n",
        "    axes[0, 1].set_xlabel('Token Length')\n",
        "    axes[0, 1].set_ylabel('Count')\n",
        "    \n",
        "    # 3. Merge progression (if we have merge data)\n",
        "    if bpe_encoder.merges:\n",
        "        merge_indices = list(range(len(bpe_encoder.merges)))\n",
        "        vocab_sizes = [len(bpe_encoder.vocab) - len(bpe_encoder.merges) + i for i in merge_indices]\n",
        "        \n",
        "        axes[1, 0].plot(merge_indices, vocab_sizes, marker='o', markersize=3)\n",
        "        axes[1, 0].set_title('Vocabulary Size Growth')\n",
        "        axes[1, 0].set_xlabel('Merge Step')\n",
        "        axes[1, 0].set_ylabel('Vocabulary Size')\n",
        "    \n",
        "    # 4. Character frequency in vocabulary\n",
        "    char_freq = Counter()\n",
        "    for token in bpe_encoder.vocab.keys():\n",
        "        for char in token:\n",
        "            char_freq[char] += 1\n",
        "    \n",
        "    top_chars = dict(char_freq.most_common(15))\n",
        "    axes[1, 1].bar(range(len(top_chars)), list(top_chars.values()))\n",
        "    axes[1, 1].set_title('Character Frequency in Vocabulary')\n",
        "    axes[1, 1].set_xlabel('Character')\n",
        "    axes[1, 1].set_ylabel('Frequency')\n",
        "    axes[1, 1].set_xticks(range(len(top_chars)))\n",
        "    axes[1, 1].set_xticklabels(list(top_chars.keys()), rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print summary statistics\n",
        "    print(f\"\\nBPE Summary Statistics:\")\n",
        "    print(f\"Total vocabulary size: {stats['total_vocab_size']}\")\n",
        "    print(f\"Number of merges performed: {stats['number_of_merges']}\")\n",
        "    print(f\"Average token length: {np.mean(token_lengths):.2f}\")\n",
        "    print(f\"Token length std: {np.std(token_lengths):.2f}\")\n",
        "    print(f\"Unique characters: {len(char_freq)}\")\n",
        "\n",
        "# Test visualization with our sample BPE encoder\n",
        "if 'bpe_encoder' in locals() and bpe_encoder.vocab:\n",
        "    print(\"Generating BPE visualization...\")\n",
        "    visualize_bpe_stats(bpe_encoder)\n",
        "else:\n",
        "    print(\"Please train a BPE encoder first to see visualizations\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Instructions for Using with Your PDF File\n",
        "\n",
        "To use this notebook with your own PDF file:\n",
        "\n",
        "1. **Place your PDF file** in the `TOKENIZATION` folder\n",
        "2. **Update the PDF path** in the cell below with your filename\n",
        "3. **Run the processing function** to train BPE on your document\n",
        "4. **Analyze the results** using the visualization functions\n",
        "\n",
        "### Example Usage:\n",
        "```python\n",
        "# Replace 'your_document.pdf' with your actual PDF filename\n",
        "pdf_path = \"your_document.pdf\"\n",
        "bpe_encoder, pytorch_tokenizer = process_pdf_with_bpe(pdf_path, vocab_size=2000)\n",
        "\n",
        "# Visualize the results\n",
        "visualize_bpe_stats(bpe_encoder)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ready to process your PDF file!\n",
        "# Uncomment and modify the following lines when you have a PDF file:\n",
        "\n",
        "# pdf_path = \"your_document.pdf\"  # Replace with your PDF filename\n",
        "# bpe_encoder, pytorch_tokenizer = process_pdf_with_bpe(pdf_path, vocab_size=2000)\n",
        "\n",
        "print(\"ðŸŽ‰ Byte Pair Encoding implementation complete!\")\n",
        "print(\"\\nFeatures implemented:\")\n",
        "print(\"âœ… Complete BPE algorithm with PyTorch integration\")\n",
        "print(\"âœ… PDF text extraction using PyPDF\")\n",
        "print(\"âœ… Vocabulary building and tokenization\")\n",
        "print(\"âœ… Encoding and decoding functionality\")\n",
        "print(\"âœ… Visualization and analysis tools\")\n",
        "print(\"âœ… Ready for your PDF files!\")\n",
        "\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"1. Add your PDF file to the TOKENIZATION folder\")\n",
        "print(\"2. Update the pdf_path variable above\")\n",
        "print(\"3. Run the process_pdf_with_bpe() function\")\n",
        "print(\"4. Explore the tokenization results!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
